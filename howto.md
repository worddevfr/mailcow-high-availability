<div align="center">
    <img src="logo.png" alt="Mailcow ISP" height="20%" width="20%" style="vertical-align: middle;">
</div>

<h1 align="center">Mailcow ISP</h1>

<p>&nbsp;</p>

<p align="center">
  <a href="https://github.com/mailcow/mailcow-dockerized" target="_blank">
    <img src="https://img.shields.io/badge/MAILCOW-FFC107?style=for-the-badge&logoColor=white" alt="Mailcow"/>
  </a>
  <a href="https://www.docker.com/" target="_blank">
    <img src="https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white" alt="Docker"/>
  </a>
  <a href="https://www.keepalived.org/" target="_blank">
    <img src="https://img.shields.io/badge/Keepalived-009688?style=for-the-badge" alt="Keepalived"/>
  </a>
  <a href="https://mariadb.com/kb/en/galera-cluster/" target="_blank">
    <img src="https://img.shields.io/badge/MariaDB%20Galera-003545?style=for-the-badge&logo=mariadb&logoColor=white" alt="MariaDB Galera"/>
  </a>
  <a href="https://www.hetzner.com/cloud" target="_blank">
    <img src="https://img.shields.io/badge/Hetzner%20Cloud-D50C2D?style=for-the-badge&logo=hetzner&logoColor=white" alt="Hetzner"/>
  </a>
  <a href="https://www.gnu.org/software/bash/" target="_blank">
    <img src="https://img.shields.io/badge/GNU%20Bash-4EAA25?style=for-the-badge&logo=gnubash&logoColor=white" alt="GNU Bash"/>
  </a>
<br><br>
</p>

<details>
<summary><strong>üá¨üáß English Version (click to expand)</strong></summary>

### üìÑ Detailed Technical Architecture

#### 1. Philosophy and Objectives

A standard Mailcow instance, while powerful, is a **Single Point of Failure (SPOF)**. A hardware failure, a faulty update, or a critical container crash is enough to bring down the entire mail service.

The goal of **Mailcow-HA** is to transform this single instance into a **resilient Active/Passive cluster**, capable of surviving most failures by automatically failing over the service to a standby node, with zero human intervention and minimal service interruption.

This solution is designed as an **orchestration layer** that integrates with a standard Mailcow installation without modifying its core, ensuring full compatibility with future Mailcow updates.

---

#### 2. The Four Pillars of the Infrastructure

The cluster's robustness is built on four fundamental components working in concert.

##### üèõÔ∏è **Pillar 1: The Application Nodes**
The cluster consists of **three identical servers (nodes) by default**, but its architecture is designed to be **scalable to 5, 7, or more nodes**. Each node hosts:
1.  A complete and ready-to-start Mailcow Dockerized installation.
2.  A MariaDB database server, member of the Galera Cluster.
3.  The cluster management service (Keepalived).

At any given time, only one of these nodes is designated as **`MASTER`** and actively handles traffic. The others are in a hot-standby **`BACKUP`** state, ready to take over.

##### üß† **Pillar 2: The Brain - The Mailcow-HA Orchestrator**
Our suite of orchestration scripts is the true conductor of the cluster. It uses **Keepalived** as an engine to manage a complex high-availability logic:
*   **Floating IP Management:** The orchestrator is solely responsible for assigning the cluster's unique public IP. This is the single entry point for all users.
*   **Application Monitoring:** An intelligent monitoring script runs at regular intervals to deeply probe the state of the Mailcow stack (running containers, health status, etc.).
*   **Action Orchestration:** Based on the monitor's verdict, the orchestrator makes decisions. If it promotes a node to `MASTER`, it runs a promotion script. If it demotes it to `BACKUP`, it runs a demotion script. In all cases, you are **alerted in real-time** when a failover begins and when it successfully completes.

##### üíæ **Pillar 3: The Resilient Database - Galera Cluster**
The database is externalized from Mailcow and managed by a **MariaDB Galera Cluster (3 nodes by default, scalable)**.
*   **Active Synchronization:** Galera ensures synchronous replication of all data. Every write to one node is instantly replicated to the others.
*   **Security and Performance:** Replication occurs over a dedicated **private network**, isolating this critical traffic and ensuring minimal latency.
*   **Scalable Dedicated Storage:** Each MariaDB node has its own dedicated block storage volume (Hetzner Volume), statically attached. These volumes are **hot-resizable**, with no service interruption, ensuring you can manage a very large number of users.

##### üóÉÔ∏è **Pillar 4: Centralized File Storage**
To ensure perfect consistency and simplify management, a **single shared block storage volume** is used to centralize all of Mailcow's "stateful" data (emails, indexes, certificates, etc.).
*   **Mechanism:** This is a "floating" resource, dynamically attached to the active `MASTER` node. It is never accessible by more than one node at a time.
*   **Benefit:** This centralization provides **enhanced ease of backup and restore**. A full backup can be performed from any node, as the data is always an exact reflection of the service's state.

---

### 3. Anatomy of an Automatic Failover (A to Z)

Here is the precise, step-by-step sequence of events when a failure occurs on the `MASTER` node.

1.  **The Failure:** A critical container (e.g., `dovecot-mailcow`) on the `MASTER` crashes.

2.  **The Detection:**
    *   **~2 seconds later**, the monitoring script detects that the container is no longer in a `running` state.
    *   It returns an error code to the orchestrator.

3.  **The Decision:**
    *   The orchestrator on the `MASTER` receives the error code. It immediately enters the `FAULT` state and relinquishes its `MASTER` role, notifying the other cluster members.

4.  **The Election:**
    *   The `BACKUP` nodes see that the `MASTER` has disappeared. The node with the highest priority elects itself as the new `MASTER`.

5.  **The Orchestration (on the new `MASTER`)**:
    *   Upon its promotion, the orchestrator runs the promotion script.
    *   This script executes its critical sequence:
        a. **Circuit Breaker Check:** It checks if a failover has already occurred on this node less than a user-defined time ago (**1 hour by default**). If so, it stops and sends an alert to prevent a failover loop.
        b. **Timestamp Update:** It records the start of the failover to grant a grace period to the monitoring system.
        c. **Resource Failover (in parallel):** It launches simultaneous API calls to reassign the **shared volume** and the **Floating IP**.
        d. **Wait and Mount:** It waits for confirmation that both operations are complete, then **mounts** the shared volume.
        e. **Service Start-up:** It starts the Mailcow Docker containers and ensures they are all fully operational.

6.  **The Grace Period:**
    *   While the promotion script is working (**in just a few seconds**, depending on machine performance), the monitoring script on the new `MASTER` is patient, as it has detected that a failover has just begun.

7.  **The Cleanup (on the old `MASTER`)**:
    *   Meanwhile, the orchestrator on the old `MASTER` (now in `BACKUP` state) runs a demotion script that cleanly stops any remaining containers and **unmounts** the volume.

8.  **Return to Normal:**
    *   On the new `MASTER`, the containers stabilize. The service is once again 100% operational. A success alert is sent to the administrator.

Rest assured, from the moment a failure is detected to the moment the service is available again, **only a few seconds elapse!**

Meanwhile, a **dual monitoring system** (an internal smart monitor and an external one via [Uptime Kuma](https://github.com/louislam/uptime-kuma)) ensures total visibility and instantly alerts the administrator without flooding them with notifications. Additionally, "garbage collector" scripts run at regular intervals to clean up any potential residues.

</details>

<br>

<details>
<summary><strong>üá´üá∑ Version Fran√ßaise (Ccliquer pour d√©plier)</strong></summary>

### üìÑ Architecture Technique D√©taill√©e

#### 1. Philosophie et Objectifs

Une instance Mailcow standard, bien que performante, constitue un **point de d√©faillance unique (SPOF)**. Une panne mat√©rielle, une erreur de mise √† jour ou un dysfonctionnement d'un conteneur critique suffit √† rendre l'ensemble du service de messagerie indisponible.

L'objectif de **Mailcow-HA** est de transformer cette instance unique en un **cluster r√©silient de type Actif/Passif**, capable de survivre √† la plupart des pannes en basculant automatiquement le service sur un n≈ìud de secours, avec une intervention humaine nulle et une interruption de service minimale.

La solution est con√ßue comme une **surcouche d'orchestration** qui s'int√®gre √† une installation Mailcow standard sans en modifier le c≈ìur, garantissant ainsi la compatibilit√© avec les futures mises √† jour de Mailcow.

---

#### 2. Les Quatre Piliers de l'Infrastructure

La robustesse du cluster repose sur quatre composants fondamentaux qui travaillent de concert.

##### üèõÔ∏è **Pilier 1 : Les N≈ìuds Applicatifs**
Le cluster est compos√© de **trois serveurs (n≈ìuds) identiques par d√©faut**, mais son architecture est con√ßue pour √™tre **extensible √† 5, 7 n≈ìuds ou plus**. Chaque n≈ìud h√©berge :
1.  Une installation compl√®te de Mailcow Dockerized, pr√™te √† d√©marrer.
2.  Un serveur de base de donn√©es MariaDB, membre du cluster Galera.
3.  Le service de gestion du cluster (Keepalived).

√Ä tout instant, un seul de ces n≈ìuds est d√©sign√© **`MASTER`** et traite activement le trafic. Les autres sont en √©tat de **`BACKUP`** (hot-standby), pr√™ts √† prendre le relais.

##### üß† **Pilier 2 : Le Cerveau - L'Orchestrateur Mailcow-HA**
Notre suite de scripts d'orchestration est le v√©ritable chef d'orchestre du cluster. Elle utilise **Keepalived** comme moteur pour g√©rer une logique de haute disponibilit√© complexe :
*   **Gestion de l'IP Flottante :** L'orchestrateur est le seul responsable de l'assignation de l'IP publique unique du cluster. C'est le point d'entr√©e de tous les utilisateurs.
*   **Surveillance Applicative :** Un script de surveillance intelligent est ex√©cut√© √† intervalle r√©gulier pour sonder en profondeur l'√©tat de la pile Mailcow (conteneurs actifs, √©tat de sant√©, etc.).
*   **Orchestration des Actions :** En fonction du verdict du moniteur, l'orchestrateur prend des d√©cisions. S'il promeut un n≈ìud en `MASTER`, il ex√©cute un script de promotion. S'il le r√©trograde en `BACKUP`, il ex√©cute un script de r√©trogradation. Dans tous les cas, vous √™tes **alert√© en temps r√©el** du d√©but et de la fin de la bascule.

##### üíæ **Pilier 3 : La Base de Donn√©es R√©siliente - Galera Cluster**
La base de donn√©es est externalis√©e de Mailcow et g√©r√©e par un **cluster MariaDB Galera (3 n≈ìuds par d√©faut, extensible)**.
*   **Synchronisation Active :** Galera assure une r√©plication synchrone de toutes les donn√©es. Chaque √©criture sur un n≈ìud est instantan√©ment r√©pliqu√©e sur les autres.
*   **S√©curit√© et Performance :** La r√©plication se fait sur un **r√©seau priv√©** d√©di√©, isolant ce trafic critique et garantissant des latences minimales.
*   **Stockage D√©di√© √âvolutif :** Chaque n≈ìud MariaDB dispose de son propre volume de stockage (Hetzner Volume), attach√© de mani√®re statique. Ces volumes sont **redimensionnables √† chaud**, sans aucune interruption de service, vous garantissant la capacit√© de g√©rer un tr√®s grand nombre d'utilisateurs.

##### üóÉÔ∏è **Pilier 4 : Le Stockage Centralis√© des Fichiers**
Pour garantir une coh√©rence parfaite et simplifier la gestion, un **unique volume de stockage bloc partag√©** est utilis√© pour centraliser toutes les donn√©es "stateful" de Mailcow (e-mails, index, certificats, etc.).
*   **M√©canisme :** Ce volume est une ressource "flottante", attach√©e dynamiquement au n≈ìud `MASTER` actif. Il n'est jamais accessible par plus d'un n≈ìud √† la fois.
*   **B√©n√©fice :** Cette centralisation garantit une **facilit√© accrue de sauvegarde et de restauration**. Une sauvegarde compl√®te peut √™tre effectu√©e depuis n'importe quel n≈ìud, car les donn√©es sont toujours le reflet exact de l'√©tat du service.

---

### 3. Anatomie d'une Bascule Automatique (de A √† Z)

Voici le d√©roulement pr√©cis, √©tape par √©tape, lorsqu'une panne survient sur le n≈ìud `MASTER`.

1.  **La Panne :** Un conteneur critique (ex: `dovecot-mailcow`) sur le `MASTER` plante.

2.  **La D√©tection :**
    *   **~2 secondes plus tard**, le script de surveillance d√©tecte que le conteneur n'est plus √† l'√©tat `running`.
    *   Il retourne un code d'erreur √† l'orchestrateur.

3.  **La D√©cision :**
    *   L'orchestrateur sur le `MASTER` re√ßoit le code d'erreur. Il entre imm√©diatement dans l'√©tat `FAULT` et abandonne son r√¥le, notifiant les autres membres du cluster.

4.  **L'√âlection :**
    *   Les n≈ìuds `BACKUP` voient que le `MASTER` a disparu. Le n≈ìud avec la plus haute priorit√© s'√©lit lui-m√™me comme nouveau `MASTER`.

5.  **L'Orchestration (sur le nouveau `MASTER`)**:
    *   D√®s sa promotion, l'orchestrateur ex√©cute le script de promotion.
    *   Celui-ci ex√©cute sa s√©quence critique :
        a. **V√©rification du Disjoncteur :** Il v√©rifie si une bascule a d√©j√† eu lieu sur ce n≈ìud il y a moins d'un temps d√©fini par l'administrateur (**1 heure par d√©faut**). Si c'est le cas, il s'arr√™te et envoie une alerte pour √©viter une boucle de basculement.
        b. **Mise √† Jour des Chronom√®tres :** Il enregistre le d√©but de la bascule pour accorder une p√©riode de gr√¢ce √† la surveillance.
        c. **Bascule des Ressources (en parall√®le) :** Il lance les appels pour r√©assigner le **volume partag√©** et l'**IP Flottante** simultan√©ment.
        d. **Attente et Montage :** Il attend la confirmation que les deux op√©rations sont termin√©es, puis il **monte** le volume partag√©.
        e. **D√©marrage des Services :** Il d√©marre les services (conteneurs Docker) de Mailcow et s'assure qu'ils sont tous en √©tat de fonctionnement.

6.  **La P√©riode de Gr√¢ce :**
    *   Pendant que le script de promotion travaille (**en √† peine quelques secondes**, selon la performance des machines), le script de surveillance du nouveau `MASTER` est patient, car il a d√©tect√© le d√©but d'une bascule.

7.  **Le Nettoyage (sur l'ancien `MASTER`)**:
    *   Pendant ce temps, l'orchestrateur sur l'ancien `MASTER` (maintenant en `BACKUP`) ex√©cute un script de r√©trogradation qui arr√™te proprement les conteneurs restants et **d√©monte** le volume.

8.  **Le Retour √† la Normale :**
    *   Sur le nouveau `MASTER`, les conteneurs se stabilisent. Le service est de nouveau 100% op√©rationnel. Une alerte de succ√®s est envoy√©e √† l'administrateur.

Rassurez-vous, entre l'instant o√π la panne est d√©tect√©e et la disponibilit√© √† nouveau du service, il ne s'√©coule **qu'√† peine quelques secondes** !

Pendant ce temps, une **double surveillance** (une interne gr√¢ce √† un monitoring intelligent et une autre externe via [Uptime Kuma](https://github.com/louislam/uptime-kuma)) garantit une visibilit√© totale et alerte instantan√©ment l'administrateur sans l'inonder de notifications. De plus, des scripts "ramasse-miettes" s'ex√©cutent √† intervalle r√©gulier pour nettoyer les r√©sidus potentiels.

</details>

